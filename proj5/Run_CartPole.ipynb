{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.0.0\n",
      "Keras 2.3.1\n",
      "gym 0.10.5\n",
      "plotly 4.1.1\n",
      "pandas 0.25.2\n",
      "numpy 1.17.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import resource\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "# keras bundled with Tensorflow 2.0 ran slower, leaked memory. got latest\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.models import Model, Sequential, load_model\n",
    "# from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import tensorflow.keras.backend as K\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# requires python 3.6\n",
    "# conda install -c akode gym\n",
    "import gym\n",
    "\n",
    "# set seeds for reproducibility\n",
    "# np.random.uniform(0,10000) 4465\n",
    "GLOBAL_SEED = 4465\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "tf.random.set_seed(GLOBAL_SEED)\n",
    "\n",
    "print(\"TensorFlow %s\" % tf.__version__)\n",
    "print(\"Keras %s\" % keras.__version__)\n",
    "print(\"gym %s\" % gym.__version__)\n",
    "print(\"plotly %s\" % plotly.__version__)\n",
    "print(\"pandas %s\" % pd.__version__)\n",
    "print(\"numpy %s\" % np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIMESTEPS = 500\n",
    "N_EPISODES = 2000\n",
    "WIN_REWARD = 10\n",
    "DISCOUNT_RATE = 0.98\n",
    "RENDER = False\n",
    "SAMPLE_SIZE = 128\n",
    "OUTPUT_DIR = 'model_output/cartpole/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'224.8 MB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show memory usage (some versions of TensorFlow gave memory issues)\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    \"\"\"given memory as int format as memory units eg KB\"\"\"\n",
    "    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Y', suffix)\n",
    "\n",
    "def memusage():\n",
    "    \"\"\"print memory usage\"\"\"\n",
    "    return sizeof_fmt(int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n",
    "\n",
    "memusage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"abstract base class for agents\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, filename=\"model\",\n",
    "                 *args, **kwargs):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.filename = filename\n",
    "        self.timestep = 0\n",
    "        self.save_interval = 10\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_model(self, *args, **kwargs):\n",
    "        \"\"\"build the relevant model\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"reset agent for start of episode\"\"\"\n",
    "        self.timestep = 0\n",
    "\n",
    "    def increment_time(self):\n",
    "        \"\"\"increment timestep counter\"\"\"\n",
    "        self.timestep += 1\n",
    "\n",
    "    def remember(self, *args, **kwargs):\n",
    "        \"\"\"store the states and rewards needed to fit the model\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, *args, **kwargs):\n",
    "        \"\"\"train the model on experience stored by remember\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, *args, **kwargs):\n",
    "        \"\"\"pick an action using model\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_score(self):\n",
    "        \"\"\"save score of each episode\"\"\"\n",
    "        self.results.append(self.timestep)\n",
    "\n",
    "    def score_episode(self, episode_num, n_episodes):\n",
    "        \"\"\"output results and save\"\"\"\n",
    "        self.save_score()\n",
    "        avglen = min(len(self.results), self.save_interval)\n",
    "        formatstr = \"{} episode {}: {}/{}, score: {}, {}-episode avg: {:.1f} Memory: {}        \"\n",
    "        print(formatstr.format(time.strftime(\"%H:%M:%S\"), len(self.results),\n",
    "                               episode_num+1, n_episodes, self.timestep, avglen,\n",
    "                               sum(self.results[-avglen:])/avglen, memusage()),\n",
    "              end=\"\\r\", flush=False)\n",
    "\n",
    "    def run_episode(self, render=RENDER):\n",
    "        \"\"\"run a full episode\"\"\"\n",
    "        global env\n",
    "\n",
    "        self.reset()\n",
    "        self.state = env.reset()\n",
    "        self.done = False\n",
    "\n",
    "        while not self.done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            self.action = self.act(self.state.reshape([1, self.state_size]))\n",
    "            self.next_state, self.reward, self.done, _ = env.step(self.action)\n",
    "            # should get extra reward for max + not done vs. max + done\n",
    "            if self.done and self.timestep == (MAX_TIMESTEPS -1):\n",
    "                self.reward += WIN_REWARD\n",
    "\n",
    "            self.remember()\n",
    "            self.state = self.next_state\n",
    "            self.increment_time()\n",
    "\n",
    "        # train\n",
    "        self.train()\n",
    "\n",
    "    def save(self, *args, **kwargs):\n",
    "        \"\"\"save agent to disk\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load(*args, **kwargs):\n",
    "        \"\"\"load agent from disk\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def view(self):\n",
    "        \"\"\"Run an episode without training, with rendering\"\"\"\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        done = False\n",
    "\n",
    "        # run an episode\n",
    "        self.timestep = 0\n",
    "        r = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = self.act(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            r += reward\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            self.timestep += 1\n",
    "        print(r)\n",
    "        env.close()\n",
    "        return self.timestep\n",
    "\n",
    "    def rlplot(self, title='Cartpole Agent Training Progress'):\n",
    "        \"\"\"plot training progress\"\"\"\n",
    "        df = pd.DataFrame({'timesteps': self.results})\n",
    "        df['avg'] = df['timesteps'].rolling(10).mean()\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=df.index,\n",
    "                                 y=df['timesteps'],\n",
    "                                 mode='markers',\n",
    "                                 name='timesteps',\n",
    "                                 marker=dict(\n",
    "                                     color='mediumblue',\n",
    "                                     size=4,\n",
    "                                 ),\n",
    "                                ))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=df.index,\n",
    "                                 y=df['avg'],\n",
    "                                 mode='lines',\n",
    "                                 line_width=3,\n",
    "                                 name='moving average'))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=dict(text=title,\n",
    "                       x=0.5,\n",
    "                       xanchor='center'),\n",
    "            xaxis=dict(\n",
    "                title=\"Episodes\",\n",
    "                linecolor='black',\n",
    "                linewidth=1,\n",
    "                mirror=True\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title=\"Completed Timesteps\",\n",
    "                linecolor='black',\n",
    "                linewidth=1,\n",
    "                mirror=True\n",
    "            ),\n",
    "            legend=go.layout.Legend(\n",
    "                x=0.01,\n",
    "                y=0.99,\n",
    "                traceorder=\"normal\",\n",
    "                font=dict(\n",
    "                    family=\"sans-serif\",\n",
    "                    size=12,\n",
    "                    color=\"black\"\n",
    "                ),\n",
    "                #bgcolor=\"LightSteelBlue\",\n",
    "                bordercolor=\"Black\",\n",
    "                borderwidth=1,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_Agent(Agent):\n",
    "    # REINFORCE policy gradient method using deep Keras NN\n",
    "    def __init__(self, state_size=4, action_size=2, learning_rate=0.0005,\n",
    "                 discount_rate=0.98, n_hidden_layers=2, hidden_layer_size=16,\n",
    "                 activation='relu', reg_penalty=0, dropout=0, filename=\"kreinforce\",\n",
    "                 verbose=True):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_space = list(range(action_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.activation = activation\n",
    "        self.reg_penalty = reg_penalty\n",
    "        self.dropout = dropout\n",
    "        self.verbose = verbose\n",
    "        self.filename = filename\n",
    "\n",
    "        self.train_model, self.predict_model = self.build_model()\n",
    "        self.results = []\n",
    "        self.save_interval = 10\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        # truncate memory\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        def custom_loss(y_true, y_pred):\n",
    "            y_pred_clip = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "            log_likelihood = y_true*K.log(y_pred_clip)\n",
    "            return K.sum(-log_likelihood*discounted_rewards)\n",
    "\n",
    "        inputs = Input(shape=(self.state_size,), name=\"Input\")\n",
    "        discounted_rewards = Input(shape=(1,), name=\"Discounted_rewards\")\n",
    "        last_layer = inputs\n",
    "\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            if self.verbose:\n",
    "                formatstr = \"layer %d size %d, %s, reg_penalty %.8f, dropout %.3f\"\n",
    "                print(formatstr % (i + 1,\n",
    "                                   self.hidden_layer_size,\n",
    "                                   self.activation,\n",
    "                                   self.reg_penalty,\n",
    "                                   self.dropout,\n",
    "                                   ))\n",
    "            # add dropout, but not on inputs, only between hidden layers\n",
    "            if i and self.dropout:\n",
    "                last_layer = Dropout(self.dropout, name=\"Dropout%02d\" % i)(last_layer)\n",
    "\n",
    "            last_layer = Dense(units=self.hidden_layer_size,\n",
    "                               activation=self.activation,\n",
    "                               kernel_initializer=glorot_uniform(),\n",
    "                               kernel_regularizer=keras.regularizers.l2(self.reg_penalty),\n",
    "                               name=\"Dense%02d\" % i)(last_layer)\n",
    "\n",
    "        outputs = Dense(self.action_size, activation='softmax', name=\"Output\")(last_layer)\n",
    "\n",
    "        train_model = Model(inputs=[inputs, discounted_rewards], outputs=[outputs])\n",
    "        train_model.compile(optimizer=Adam(lr=self.learning_rate), loss=custom_loss)\n",
    "\n",
    "        predict_model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "        if self.verbose:\n",
    "            print(predict_model.summary())\n",
    "\n",
    "        return train_model, predict_model\n",
    "\n",
    "    def act(self, state):\n",
    "        probabilities = self.predict_model.predict(state)\n",
    "        action = np.random.choice(self.action_space, p=probabilities[0])\n",
    "        return action\n",
    "\n",
    "    def remember(self):\n",
    "        self.state_memory.append(self.state)\n",
    "        self.action_memory.append(self.action)\n",
    "        self.reward_memory.append(self.reward)\n",
    "\n",
    "    def train(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "\n",
    "        actions = np.zeros([len(action_memory), self.action_size])\n",
    "        actions[np.arange(len(action_memory)), action_memory] = 1\n",
    "\n",
    "        discounted_rewards = np.zeros_like(reward_memory)\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(len(reward_memory))):\n",
    "            cumulative_rewards = cumulative_rewards * self.discount_rate + reward_memory[i]\n",
    "            discounted_rewards[i] = cumulative_rewards\n",
    "\n",
    "        # standardize\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards) if np.std(discounted_rewards) > 0 else 1\n",
    "\n",
    "        # train\n",
    "        cost = self.train_model.train_on_batch([state_memory, discounted_rewards], actions)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def save(self):\n",
    "        \"save agent: pickle self and use Keras native save model\"\n",
    "        fullname = \"%s%s%04d\" % (OUTPUT_DIR, self.filename, len(self.results))\n",
    "        self.predict_model.save(\"%s_predict.h5\" % fullname)\n",
    "        # can't save / load train model due to custom loss\n",
    "        pickle.dump(self, open(\"%s.p\" % fullname, \"wb\"))\n",
    "\n",
    "    def load(filename, memory=True):\n",
    "        \"load saved agent\"\n",
    "        self = pickle.load(open(\"%s.p\" % filename, \"rb\"))\n",
    "        self.predict_model = load_model(\"%s_predict.h5\" % filename)\n",
    "        print(\"loaded %d results, %d rows of memory, epsilon %.4f\" % (len(self.results),\n",
    "                                                                      len(self.memory),\n",
    "                                                                      self.epsilon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "layer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\n",
      "layer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense00 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "Dense01 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/druce/anaconda3/envs/python36/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning:\n",
      "\n",
      "Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "\n",
      "/Users/druce/anaconda3/envs/python36/lib/python3.6/site-packages/keras/engine/saving.py:341: UserWarning:\n",
      "\n",
      "No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(int(np.random.uniform(10000)))\n",
    "\n",
    "agent = REINFORCE_Agent(state_size=env.observation_space.shape[0],\n",
    "                        action_size=env.action_space.n,\n",
    "                        learning_rate=0.0005,\n",
    "                        discount_rate=0.99,)\n",
    "agent.predict_model = load_model(\"reinforce.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
